{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrE9FZvJo_1J"
      },
      "source": [
        "# **UGain ML Tree-based Models Exercise Solutions Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Welcome dear reader to the tree-based model exercises. In this notebook, we will cover decision trees, pruning, ensemble methods, hyperparameters and variable importance.\n",
        "\n",
        "To start with, we need to import several libraries that will be needed to run the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzyz8bLHpHLI"
      },
      "source": [
        "## **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cK5O2zIAo_z"
      },
      "outputs": [],
      "source": [
        "# Basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
        "\n",
        "# Sklearn\n",
        "## Data\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## Models\n",
        "from sklearn import tree\n",
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Utility functions\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "\n",
        "## Model Explaination\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "## Metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# XGBoost\n",
        "import xgboost\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next up, we will load the dataset that will be used for the exercises. This dataset is the diabetes dataset, which is build into sklearn by default. It contains several variables of diabetic patients. The goal will be to see if some of these variables could be used to predict diabetes.\n",
        "\n",
        "The complete description can be read using diabetes_data['DESCR']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNm_6IWVpM7J"
      },
      "source": [
        "## **Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_i2qQJPAtwk",
        "outputId": "38e4df1d-e179-427e-bd50-9fe69132d23c"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "diabetes_data = load_diabetes()\n",
        "predictors = diabetes_data['data']\n",
        "labels = diabetes_data['target']\n",
        "\n",
        "# Print description of the dataset\n",
        "print(diabetes_data['DESCR'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we will split this dataset into a training set and a test set. Conveniently, sklearn has a simple function called train_test_split which can do this for us. As inputs, it takes the X values and the y values (which we call predictors and labels here) and a random state. This random state ensures that anyone who runs the code gets the exact same split. Changing the seed value will result in a different split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IclnVblmCYPC"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "seed = 0\n",
        "\n",
        "# Train - Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(predictors, \n",
        "                                                    labels, \n",
        "                                                    random_state=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can specify the size of both datasets if we want to, or use stratification. For more info, you can look up the help page of sklearn.train_test_split\n",
        "\n",
        "Now that we have our test set, we can start the exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P4ETF2DpPHq"
      },
      "source": [
        "## **Exercises**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMQO_lLxn1CK"
      },
      "source": [
        "### **Exercise 1a: Fit Decision Tree (Regression)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First up, we will fit a decision tree to the data. Once again, we can use random state to ensure the same result is always obtained. Fitting a decision tree happens in two steps:\n",
        "- Define the model with its parameters. For now, we will use the default values.\n",
        "- Fit this model to the training data\n",
        "\n",
        "Once this is done, the model can be evaluated by calculating the loss. The rmse can be calculated by:\n",
        "- Calculating the predicted values using model.predict()\n",
        "- Subtracting the real values from the predicted ones and squaring the result\n",
        "- Divide by the number of observations\n",
        "- Summing up all these values and taking the square root to get the rmse\n",
        "\n",
        "Then we can print the result (with some formatting to make it look nice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGH0jJN6rbsJ",
        "outputId": "9bda1e28-9678-4951-88c3-c4f7d26d1e75"
      },
      "outputs": [],
      "source": [
        "# Create decision tree regressor object\n",
        "decision_tree_regressor = tree.DecisionTreeRegressor(random_state=seed)\n",
        "\n",
        "# Fit the training data to the regressor\n",
        "decision_tree_regressor = decision_tree_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Calculate root mean square error of the train and test sets\n",
        "train_rmse = (np.sum((decision_tree_regressor.predict(X_train)-y_train)**2)/len(y_train))**0.5\n",
        "test_rmse = (np.sum((decision_tree_regressor.predict(X_test)-y_test)**2)/len(y_test))**0.5\n",
        "\n",
        "# Verbose\n",
        "print(\"Train set root mean squared error is: {} and test set root mean squared error is: {}\".format(round(train_rmse, 4), \n",
        "                                                                                                    round(test_rmse, 4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njnxVPZmn988"
      },
      "source": [
        "### **Exercise 1b: Search for the Best Cost-Complexity Pruning (alpha)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great. Now we can take a look at the cost complexity pruning path. Decision trees have a tendency to overfit to the data. As you might have noticed, the training error is much lower than the test error. To avoid overfitting, we can prune our trees. First, let's take a look at ccp "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call built-in method to compute the pruning path during Minimal Cost-Complexity Pruning.\n",
        "path = decision_tree_regressor.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can use these to try and find good parameters for the model. However, to do this, we need a way to evaluate models with different parameters. This requires a formal loss function. Therefore, we make a little function to get a rmse loss function. We then use make_scorer to make this a formal loss function which can be used by sklearn. Remember to indicate that we wish to **minimize** the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_rmse(model, predictors, labels):\n",
        "  predictions = model.predict(predictors)\n",
        "  rmse = mean_squared_error(labels, predictions, squared=False)\n",
        "  return rmse\n",
        "\n",
        "def rmse_loss(true_labels, pred_labels):\n",
        "  return mean_squared_error(true_labels, pred_labels, squared=False)\n",
        "\n",
        "score_function_decision_tree = make_scorer(rmse_loss, greater_is_better=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great. Now we can get started. First, we define a grid and then we use the gridsearchCV function to find optimal parameters. This method uses cross-validation. Cross-validation includes resampling and sample splitting methods that use different portions of the data to test and train a model on different iterations. This avoids overfitting.\n",
        "\n",
        "We use a cv of 5, meaning the model is fit 5 times and we use 4/5th of the data for training and 1/5th for validation (cycling through until all combinations have been used)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b021LF0Xr0E5",
        "outputId": "23652f97-0366-4622-99c1-bc12e8ed7952"
      },
      "outputs": [],
      "source": [
        "# Define parameter space to search\n",
        "param_grid = { \n",
        "    'ccp_alpha': ccp_alphas,}\n",
        "\n",
        "# Create decision tree regressor object\n",
        "decision_tree_regressor = tree.DecisionTreeRegressor(random_state=seed)\n",
        "\n",
        "# Perform grid search in the defined parameter space with cross validation\n",
        "CV_decision_tree_regressor = GridSearchCV(estimator=decision_tree_regressor, \n",
        "                                          param_grid=param_grid, \n",
        "                                          cv= 5, \n",
        "                                          scoring=score_function_decision_tree)\n",
        "CV_decision_tree_regressor.fit(predictors, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can take a look at the best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verbose best parameters from the GridSearchCV\n",
        "print('Best Parameters:', CV_decision_tree_regressor.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great. Now let us fit a final model using these parameters and see what happens to the rmse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit decision tree regressor model with best parameters\n",
        "decision_tree_regressor = tree.DecisionTreeRegressor(random_state=seed,\n",
        "                                                       **CV_decision_tree_regressor.best_params_)\n",
        "decision_tree_regressor = decision_tree_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Calculate root mean square error of the train and test sets\n",
        "train_rmse = np.sum(((decision_tree_regressor.predict(X_train)-y_train)**2)/len(y_train))**0.5\n",
        "test_rmse = np.sum(((decision_tree_regressor.predict(X_test)-y_test)**2)/len(y_test))**0.5\n",
        "\n",
        "# Verbose\n",
        "print(\"Train set root mean squared error is: {} and test set root mean squared error is: {}\".format(round(train_rmse, 4), \n",
        "                                                                                                    round(test_rmse, 4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the test set performs much better now (lower rmse). The training set is also much more in line with the test set, indicating less overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siwKWc8DoDdh"
      },
      "source": [
        "### **Exercise 2a: Fit an Ensemble Model (Regression)**\n",
        "\n",
        "Now we will fit an ensemble model. We will select xgboost, which is an implementation of a random forest (an emsemble of decision trees).\n",
        "\n",
        "If you want to use a different model or read more about this, here are some helpful links:\n",
        "1. https://scikit-learn.org/stable/modules/ensemble.html\n",
        "2. https://xgboost.readthedocs.io/en/stable/python/python_api.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "xgboost works the same way as a decision tree. First, define an object with the right parameters and then fit it to the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f31Q_EPYwy2K",
        "outputId": "9b328c18-5de2-4fdd-b67e-643c1c1f06cc"
      },
      "outputs": [],
      "source": [
        "# Create a XGBoost regressor object\n",
        "xgboost_regressor = xgboost.XGBRegressor(objective ='reg:squarederror')\n",
        "\n",
        "# Fit the training data to the XGBoost regressor\n",
        "xgboost_regressor = xgboost_regressor.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we once again calculate rmse and print it to the screen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate root mean square error of the train and test sets\n",
        "train_rmse = get_rmse(xgboost_regressor, X_train, y_train)\n",
        "test_rmse = get_rmse(xgboost_regressor, X_test, y_test)\n",
        "\n",
        "#or manually\n",
        "train_rmse = np.sum(((xgboost_regressor.predict(X_train)-y_train)**2)/len(y_train))**0.5\n",
        "test_rmse = np.sum(((xgboost_regressor.predict(X_test)-y_test)**2)/len(y_test))**0.5\n",
        "\n",
        "\n",
        "print(\"Train set root mean squared error is: {} and test set root mean squared error is: {}\".format(round(train_rmse, 4), \n",
        "                                                                                                    round(test_rmse, 4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or9sU2MhoPC6"
      },
      "source": [
        "### **Exercise 2b: Search Hyperparameter Space of your Choice of Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once again, the model isn't performing all too well. This is because we used default parameters. To boost performance, we will once again use cross-validation to evaluate a grid of parameters until we find the optimal set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8clxC6hyjjy",
        "outputId": "41355a93-2149-423d-ab80-19ef313da3b7"
      },
      "outputs": [],
      "source": [
        "# Define parameter space to search\n",
        "param_grid = {\n",
        "    'n_estimators': [20, 50, 100],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'learning_rate': [0.1, 0.3, 0.5, 0.7],\n",
        "    'min_child_weight': [1, 10, 100, 200, 500],\n",
        "    'colsample_bytree': [0.1, 0.3, 0.7],\n",
        "}\n",
        "\n",
        "# Create XGBoost regressor object\n",
        "xgboost_regressor = xgboost.XGBRegressor(objective ='reg:squarederror')\n",
        "\n",
        "# Perform grid search in the defined parameter space with cross validation\n",
        "CV_xgboost_regressor = GridSearchCV(estimator=xgboost_regressor, \n",
        "                                    param_grid=param_grid, \n",
        "                                    cv= 5, \n",
        "                                    scoring=score_function_decision_tree)\n",
        "CV_xgboost_regressor.fit(predictors, labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let us take a look at the optimal parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verbose best parameters from the GridSearchCV\n",
        "print('Best Parameters:', CV_xgboost_regressor.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then fit a model with these optimal parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit XGBoost regressor model with best parameters\n",
        "xgboost_regressor = xgboost.XGBRegressor(objective ='reg:squarederror',\n",
        "                                         **CV_xgboost_regressor.best_params_)\n",
        "xgboost_regressor = xgboost_regressor.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we calculate rmse again and compare to the original model. Has the performance improved? What about overfitting?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Calculate root mean square error of the train and test sets\n",
        "train_rmse = get_rmse(xgboost_regressor, X_train, y_train)\n",
        "test_rmse = get_rmse(xgboost_regressor, X_test, y_test)\n",
        "\n",
        "# Verbose\n",
        "print(\"Train set root mean squared error is: {} and test set root mean squared error is: {}\".format(round(train_rmse, 4), \n",
        "                                                                                                    round(test_rmse, 4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFuoGF2dS6Bs"
      },
      "source": [
        "### **Exercise 2c: Plot: Ensemble Regressor Feature Importance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly, let us take a look at the importance of specific features. Remember that the original goal of all this was to get information on which variables would be useful to predict diabetes. There is a built-in method to do this called permutation importance. In simple words, it will calculate how much worse a model performs when one variable is left out. Whichever variable has the biggest impact is likely the most relevant for predictions. For this, we use the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "F7ccXTv4TIDr",
        "outputId": "e458298b-1a2c-4d72-e867-7b19a9aef041"
      },
      "outputs": [],
      "source": [
        "# Perform permutation feature importance using the best ensemble model\n",
        "permutation_importance_result = permutation_importance(xgboost_regressor, \n",
        "                                                       X_test, \n",
        "                                                       y_test, \n",
        "                                                       n_repeats=10, \n",
        "                                                       random_state=seed,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To evaluate this more easily, we will put it into a dataframe and make a little plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the mean and standard deviation of the feature importances from the results and create Pandas Dataframe\n",
        "forest_importances = pd.DataFrame({\"importances\" : permutation_importance_result.importances_mean, \n",
        "                                   \"stdev\" : permutation_importance_result.importances_std }, \n",
        "                                   index=diabetes_data['feature_names']).sort_values(\"importances\", ascending=False).iloc[:5]\n",
        "\n",
        "# Plot the feature importances\n",
        "fig, ax = plt.subplots(figsize=(15,8))\n",
        "forest_importances[\"importances\"].plot.bar(yerr=forest_importances.stdev, ax=ax)\n",
        "ax.set_title(\"Feature importances using permutation on test data\")\n",
        "ax.set_ylabel(\"Mean RMSE decrease\")\n",
        "ax.set_ylim(bottom=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on this plot, we can see which variables are the most important to predicting diabetes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
