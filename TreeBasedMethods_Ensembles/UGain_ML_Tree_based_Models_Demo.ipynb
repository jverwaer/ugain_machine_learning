{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H0kp5Nk-Uiu"
      },
      "source": [
        "# Tree-based methods and ensembles\n",
        "\n",
        "@author: Jan Verwaeren - Arne Deloose - Nusret Ipek\n",
        "\n",
        "@course: Machine learning: van theorie tot praktijk\n",
        "\n",
        "This notbook contains illustrations of the most prominent tree-based methods and ensemble methods used in the ML field:\n",
        "- CART: classification and regression trees (Breiman 1984)\n",
        "- Random forests\n",
        "- Gradient Boosting (XGBoost)\n",
        "- Auto-ml\n",
        "- Isolation forests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP3saUG2-fsO"
      },
      "source": [
        "## Import libraries\n",
        "\n",
        "******\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gxiom-qM-LvD"
      },
      "outputs": [],
      "source": [
        "# Basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "\n",
        "# Sklearn\n",
        "## Data\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## Models\n",
        "from sklearn import tree\n",
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "## Model Explaination\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "\n",
        "## Metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# XGBoost\n",
        "import xgboost\n",
        "\n",
        "# Plotting\n",
        "import graphviz\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pKCSXty-plm"
      },
      "source": [
        "## 1. The Wisonsin breast cancer dataset\n",
        "\n",
        "******\n",
        "\n",
        "In this demo notebook, the Wisconsin breast cancer dataset is used. Several features derived from microscopic images of tumor tissue are used to predict whether a tumor is Malignant (212 instances) or Benign (357 instances). The problem is thus a binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76KkfmIq_AxG",
        "outputId": "c48fed25-77dd-4ee8-f60e-a74ec1ec5153"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "breast_cancer_data = load_breast_cancer()\n",
        "predictors = breast_cancer_data['data']\n",
        "labels = breast_cancer_data['target']\n",
        "\n",
        "# Print description of the dataset\n",
        "print(breast_cancer_data['DESCR'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz07WfXpAWS-"
      },
      "source": [
        "The data are split in train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QjYTuF4wAVSG"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "seed = 0\n",
        "\n",
        "# Train - Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(predictors, \n",
        "                                                    labels, \n",
        "                                                    random_state=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayqf5pTy_diV"
      },
      "source": [
        "## 2. CART - classification and regression trees\n",
        "\n",
        "******\n",
        "\n",
        "Traditional decision trees are implemented by the `DecisionTreeClassifier` and `DecisionTreeRegressor` classes in the submodule `tree` of scikit learn. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o5nyl1AZMxu"
      },
      "source": [
        "### 2.1 Decision tree induction (learning a tree)\n",
        "\n",
        "Decision trees can be trained and used using the default Scikit Learn API by:\n",
        "- (1) creating an instance of the `DecisionTreeClassifier` class, \n",
        "- (2) call the `fit` method to train the model and \n",
        "- (3) call the `predict` method to predict the labels on test data. \n",
        "\n",
        "NOTE: The `DecisionTreeRegressor` class implements a tree induction algorithm for regression (not shown here)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iBxgg2tIkyK",
        "outputId": "5f8da1e4-10f7-4b0d-e8df-a9d4c5ee21f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set accuracy is: 1.0 and test set accuracy is: 0.8811\n"
          ]
        }
      ],
      "source": [
        "# Create decision tree classifier object\n",
        "decision_tree_classifier = tree.DecisionTreeClassifier(random_state=seed)\n",
        "\n",
        "# Fit the training data to the classifier\n",
        "decision_tree_classifier = decision_tree_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Calculate accuracy of the train and test sets\n",
        "train_predictions = decision_tree_classifier.predict(X_train)\n",
        "test_predictions = decision_tree_classifier.predict(X_test)\n",
        "print(\"Train set accuracy is: {} and test set accuracy is: {}\".format(round(accuracy_score(y_train, train_predictions), 4),\n",
        "                                                                      round(accuracy_score(y_test, test_predictions), 4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQuFSVDCdvtf"
      },
      "source": [
        "### 2.2 Cost-complexity pruning\n",
        "\n",
        "Cost-complexity pruning allows you to gradually prune a tree using the cost-complexity criterion. The hyper-parameter $\\alpha$ (in sklearn `cpp_alpha`) in the cost-complexity pruning acts as a regularization parameter. Larger values will result in simpler (shallow) trees.\n",
        "\n",
        "The code fragment below sweeps over a range of values for $\\alpha$ to investigate the effect of $\\alpha$ on a test set. Interestingly, due to the discrete nature of decision trees, the optimal tree according to the cost-complexity criterion will depend on $\\alpha$ in a discrete manner. The critical $\\alpha$-values at which the cost-complexity-wise optimal tree will change can be pre-computed using the method `cost_complexity_pruning_path` of the tree instance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p5GdK6nLjMP",
        "outputId": "f870675b-b571-4010-ab3b-ea8983c97228"
      },
      "outputs": [],
      "source": [
        "# Call built-in method to compute the pruning path during Minimal Cost-Complexity Pruning.\n",
        "path = decision_tree_classifier.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# For every complexity parameter computed (cpp_alpha), fit a decision tree \n",
        "\n",
        "## Declare empty lists to store values\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "decision_tree_classifier_list = []\n",
        "\n",
        "## Loop for every cost complexity pruning parameter\n",
        "for idx, ccp_alpha in enumerate(ccp_alphas):\n",
        "    ## Declare decision tree with cost complexity parameter\n",
        "    decision_tree_classifier = tree.DecisionTreeClassifier(random_state=seed, \n",
        "                                                           ccp_alpha=ccp_alpha)\n",
        "    \n",
        "    ## Fit the decision tree\n",
        "    decision_tree_classifier.fit(X_train, y_train)\n",
        "\n",
        "    ## Calculate results and save the fitted decision tree\n",
        "    train_predictions = decision_tree_classifier.predict(X_train)\n",
        "    test_predictions = decision_tree_classifier.predict(X_test)\n",
        "    train_accuracy_list.append(accuracy_score(y_train, train_predictions))\n",
        "    test_accuracy_list.append(accuracy_score(y_test, test_predictions))\n",
        "    decision_tree_classifier_list.append(decision_tree_classifier)\n",
        "\n",
        "    ## Verbose \n",
        "    str_text = \"Number of nodes in the tree number {}: \\t {} \\t-> ccp_alpha: {} \\t-> train_accuracy: {} \\t-> test_accuracy: {}\"\n",
        "    print(str_text.format(idx+1, \n",
        "                          decision_tree_classifier.tree_.node_count, \n",
        "                          str(round(ccp_alpha, 4)).ljust(6, '0'),\n",
        "                          str(round(accuracy_score(y_train, train_predictions), 4)).ljust(6, '0'),\n",
        "                          str(round(accuracy_score(y_test, test_predictions), 4)).ljust(6, '0')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqsROHfMgsFE"
      },
      "source": [
        "The effect of $\\alpha$ on the train/test accuracy can be computed and visualized as shown below. Note the typical shape of the accuracy on the test-set that often occurs during hyper-parameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "ouKlQcmaeMR-",
        "outputId": "c5fc6b47-af9c-40c6-a4ae-edff6e3da524"
      },
      "outputs": [],
      "source": [
        "# Declare figure and set parameters\n",
        "fig, ax = plt.subplots(figsize=(15,8))\n",
        "ax.set_xlabel(\"cpp_alpha\")\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
        "\n",
        "# Plot training data (excluding last cpp_alpha due to very low scores)\n",
        "ax.plot(ccp_alphas[:-1], train_accuracy_list[:-1], marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
        "\n",
        "# Plot test data (excluding last cpp_alpha due to very low scores)\n",
        "ax.plot(ccp_alphas[:-1], test_accuracy_list[:-1], marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n",
        "\n",
        "# Display the plot\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fdf7ibhh-D1"
      },
      "source": [
        "The code framgent below selects the best value for $\\alpha$ and fits a final decision tree.\n",
        "\n",
        "NOTE that tuning and testing rely on the same dataset here (for simplicty/illustration purposes). Ideally, the optimal value for $\\alpha$ is obtained using cross validation on the training set to avoid data leakage. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im7650D_gF0O",
        "outputId": "4097ff12-a2d0-4a68-9d60-9907f5259899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set accuracy is: 0.9648 and test set accuracy is: 0.9371\n"
          ]
        }
      ],
      "source": [
        "# Create decision tree classifier object\n",
        "decision_tree_classifier = tree.DecisionTreeClassifier(random_state=seed, ccp_alpha=ccp_alphas[8])\n",
        "\n",
        "# Fit the training data to the classifier\n",
        "decision_tree_classifier = decision_tree_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Calculate accuracy of the train and test sets\n",
        "train_predictions = decision_tree_classifier.predict(X_train)\n",
        "test_predictions = decision_tree_classifier.predict(X_test)\n",
        "print(\"Train set accuracy is: {} and test set accuracy is: {}\".format(round(accuracy_score(y_train, train_predictions), 4),\n",
        "                                                                      round(accuracy_score(y_test, test_predictions), 4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-jM246CiJX0"
      },
      "source": [
        "Finally, the optimal tree can be visualized as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "eVRs2Yudgk6X",
        "outputId": "d97951a2-07e4-46db-de74-3cc79d92e36a"
      },
      "outputs": [],
      "source": [
        "# Plot the fitted decision tree using GraphViz library\n",
        "dot_data = tree.export_graphviz(decision_tree_classifier, \n",
        "                                feature_names=breast_cancer_data.feature_names,  \n",
        "                                class_names=breast_cancer_data.target_names,  \n",
        "                                filled=True, \n",
        "                                rounded=True,  \n",
        "                                special_characters=True)  \n",
        "\n",
        "# Display the tree plot\n",
        "display(graphviz.Source(dot_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fROiIc23iSlN"
      },
      "source": [
        "## 3. Learning ensembles\n",
        "\n",
        "******\n",
        "\n",
        "An ensemble can most easily be interpreted as a group (a set, a sequence) of models that are combined to obtain a prediction. Most ensembles rely on bagging, boosting or stacking techniques (or combinations thereof). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2wptr6pi2ps"
      },
      "source": [
        "### 3.1 Random Forests\n",
        "\n",
        "Random forests is a simple, yet powerful ensemble method that relies on the construction of multiple decision trees on bootstrap replicates of a training set. The `ensemble` submodule of sklearn contains the classes `RandomForestClassifier` and `RandomForestRegressor` that implement random forests. The most important hyper-parameters are:\n",
        " * `n_estimators`: number of trees in the forest\n",
        " * `max_features`: the number of features to randomly select during *subspace projection* (the falue `'sqrt'` sets this parameter equal to the (rounded) square root of the total number of features)\n",
        " * `max_depth`: maximal depth of a tree\n",
        " * `min_samples_split`: the minimal number of instances in a node required to allow it to split further\n",
        "\n",
        " The code sample below fits a Random Forest classifier using defaults for the hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDNqu3XV1xYq",
        "outputId": "cb864cb4-4f9b-414e-f057-4a1b79168bbe"
      },
      "outputs": [],
      "source": [
        "# Create random forest classifier object\n",
        "random_forest_classifier = ensemble.RandomForestClassifier(warm_start=True,\n",
        "                                                           oob_score=True,\n",
        "                                                           max_features='sqrt',\n",
        "                                                           random_state=seed,)\n",
        "\n",
        "# Fit the training data to the classifier\n",
        "random_forest_classifier = random_forest_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Calculate accuracy of the train and test sets\n",
        "train_predictions = random_forest_classifier.predict(X_train)\n",
        "test_predictions = random_forest_classifier.predict(X_test)\n",
        "print(\"Train set accuracy is: {} and test set accuracy is: {}\".format(round(accuracy_score(y_train, train_predictions), 6),\n",
        "                                                                      round(accuracy_score(y_test, test_predictions), 6)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJV1I5OG9xfu"
      },
      "source": [
        "### 3.2 Illustration: effect of n_estimators and max_features on the accuracy\n",
        "\n",
        "The code fragment below illustrates the effect of the number of n_estimators and max_features on the accuracy of the resulting Random Forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "sP-sJsFx2upV",
        "outputId": "bf2fba5c-e828-43bd-bfab-4203ef40e3bd"
      },
      "outputs": [],
      "source": [
        "# Declare function to return random forest classifier with given parameters\n",
        "def rf_classifier(max_features, seed):\n",
        "  return ensemble.RandomForestClassifier(warm_start=True,\n",
        "                                         oob_score=True,\n",
        "                                         max_features=max_features,\n",
        "                                         random_state=seed,)\n",
        "\n",
        "# Declare figure and set labels\n",
        "fig, ax = plt.subplots(figsize=(15,8))\n",
        "ax.set_xlabel(\"n_estimators\")\n",
        "ax.set_ylabel(\"OOB error rate\")\n",
        "\n",
        "# Loop parameter space of max estimators and number of trees to get out of bag error scores\n",
        "for max_estimator in [1, 5, 10, 30]:\n",
        "  oob_score_list= []\n",
        "  random_forest_classifier = rf_classifier(max_estimator, 123)\n",
        "\n",
        "  for n_estimators in range(15, 501, 10):\n",
        "    random_forest_classifier.set_params(n_estimators=n_estimators)\n",
        "    random_forest_classifier = random_forest_classifier.fit(predictors, labels)\n",
        "    oob_error = 1 - random_forest_classifier.oob_score_\n",
        "    oob_score_list.append(oob_error)\n",
        "  ax.plot(list(range(15, 501, 10)), oob_score_list, label=str(max_estimator))\n",
        "\n",
        "# Display the plot\n",
        "ax.legend(loc=\"upper right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vYgRaARD-I1"
      },
      "source": [
        "### 3.2 Hyperparameter tuning for Random Forests\n",
        "\n",
        "The class `GridSearchCV` implements grid search for all models available in sklearn. It requires:\n",
        " * `param_grid`: A parameter grid to search over encoded as a dictionary where the *keys* are the names of the hyperparameters to tune and the *values* are lists that contain the values each hyperparameters should take.\n",
        " * `estimator`: the model to tune\n",
        " * `cv`: the number of folds (or a more specific cross-validation strategy, see docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyC59upG-eE1",
        "outputId": "1d1acc52-0a2e-4587-c843-84920806a533"
      },
      "outputs": [],
      "source": [
        "# Define parameter space to search\n",
        "param_grid = { \n",
        "    'max_features': [3, 10, 30],\n",
        "    'max_depth': [5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10],}\n",
        "\n",
        "# Create random forest classifier object\n",
        "random_forest_classifier = ensemble.RandomForestClassifier(n_estimators=300,\n",
        "                                                           oob_score=True, \n",
        "                                                           n_jobs=-1,\n",
        "                                                           random_state=seed,) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code fragment below performs the grid search by calling the `fit` method of a `GridSearchCV` object. Note that the `GridSearchCV` object behaves as a sklearn model. After calling `fit`, the `GridSearchCV` object behaves as a model that is trained using the optimal values for the hyper-parameters on the training set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'max_depth': 10, 'max_features': 1, 'min_samples_split': 5}\n",
            "Train set accuracy is: 0.995305 and test set accuracy is: 0.937063\n"
          ]
        }
      ],
      "source": [
        "# Perform grid search in the defined parameter space with cross validation (3 fold) -> in total 2*3*3*3 = 54 model fits \n",
        "CV_random_forest_classifier = GridSearchCV(estimator=random_forest_classifier,\n",
        "                                           param_grid=param_grid,\n",
        "                                           cv= 5)\n",
        "\n",
        "CV_random_forest_classifier.fit(X_train, y_train)\n",
        "\n",
        "# print best parameters\n",
        "print('Best Parameters:', CV_random_forest_classifier.best_params_)\n",
        "\n",
        "# Calculate accuracy of the best random forest classifier found by grid search\n",
        "train_predictions = CV_random_forest_classifier.predict(X_train)\n",
        "test_predictions = CV_random_forest_classifier.predict(X_test)\n",
        "print(\"Train set accuracy is: {} and test set accuracy is: {}\".format(round(accuracy_score(y_train, train_predictions), 6),\n",
        "                                                                      round(accuracy_score(y_test, test_predictions), 6)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0IU_d9tELI1"
      },
      "source": [
        "Alternatively, a final model can be built explicitly using the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXSo-GKWELlp",
        "outputId": "a24c8489-6941-47d1-a82a-b0f37f6a1041"
      },
      "outputs": [],
      "source": [
        "# Fit random forest model with best parameters\n",
        "random_forest_classifier = ensemble.RandomForestClassifier(n_estimators=300,\n",
        "                                                           oob_score=True, \n",
        "                                                           random_state=seed,\n",
        "                                                           **CV_random_forest_classifier.best_params_,) \n",
        "random_forest_classifier = random_forest_classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcPXL519BTF8"
      },
      "source": [
        "### 3.3 Feature importances\n",
        "\n",
        "Feature importance is an umbrella term for a set of methods that compute how much a particular features contributes to a trained model. It thus quantifies how important a feature is for the model. The code fragment below illustrates how *permutation importance* can be computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform permutation feature importance using the best random forest model\n",
        "permutation_importance_result = permutation_importance(random_forest_classifier, \n",
        "                                                       X_test, \n",
        "                                                       y_test, \n",
        "                                                       n_repeats=10, \n",
        "                                                       random_state=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting object contains the importances and their estimated standard deviations, often visualized using a bar chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "-7Dj2iZJEjAm",
        "outputId": "dcb1f451-423b-471f-fb65-22cd1fe0678d"
      },
      "outputs": [],
      "source": [
        "# Extract the mean and standard deviation of the feature importances from the importance object and wrap the in a Pandas Dataframe\n",
        "forest_importances = pd.DataFrame({\"importances\" : -permutation_importance_result.importances_mean, \n",
        "                                   \"stdev\" : permutation_importance_result.importances_std }, \n",
        "                                   index=breast_cancer_data['feature_names']).sort_values(\"importances\", ascending=False).iloc[:8]\n",
        "\n",
        "# Plot the feature importances\n",
        "fig, ax = plt.subplots(figsize=(15,8))\n",
        "forest_importances[\"importances\"].plot.bar(yerr=forest_importances.stdev, ax=ax)\n",
        "ax.set_title(\"Feature importances using permutation on test data\")\n",
        "ax.set_ylabel(\"Mean accuracy decrease\")\n",
        "ax.set_ylim(bottom=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l29c-VrLHpUU"
      },
      "source": [
        "### 3.4 Individual Partial Dependence (ICE) plots\n",
        "\n",
        "The code fragments below computes the individual partial dependencies and visualizes them in a partial dependence plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "sakvvAEEE9_k",
        "outputId": "f4724b16-4567-4828-f0d7-8854ef1ae02f"
      },
      "outputs": [],
      "source": [
        "# Create Pandas DataFrame for the test predictors\n",
        "X_test_df = pd.DataFrame(X_test, columns=breast_cancer_data['feature_names'])\n",
        "\n",
        "# Plot individual partial dependency of selected features \n",
        "fig, ax = plt.subplots(figsize=(20, 8))\n",
        "PartialDependenceDisplay.from_estimator(random_forest_classifier, \n",
        "                                        X_test_df, \n",
        "                                        features = ['worst radius', 'worst perimeter', 'mean concave points'], \n",
        "                                        kind='both', \n",
        "                                        ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ajBnJ6iDQ5b"
      },
      "source": [
        "### **Gradient Boosting Classifier with Basic Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOer2IsCDYaF",
        "outputId": "ec8de292-c593-4972-fa61-0f329879e235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set accuracy is: 1.0 and test set accuracy is: 0.958042\n"
          ]
        }
      ],
      "source": [
        "# Create a XGBoost classifier object\n",
        "gradient_boosting_classifier = ensemble.GradientBoostingClassifier(n_estimators=300, \n",
        "                                                                   learning_rate=0.1,\n",
        "                                                                   max_depth=3, \n",
        "                                                                   subsample=1,\n",
        "                                                                   min_samples_split=2,\n",
        "                                                                   min_samples_leaf=1,\n",
        "                                                                   max_features='sqrt',\n",
        "                                                                   random_state=seed,)\n",
        "\n",
        "# Fit the training data to the classifier\n",
        "gradient_boosting_classifier = gradient_boosting_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Calculate accuracy of the train and test sets\n",
        "train_predictions = gradient_boosting_classifier.predict(X_train)\n",
        "test_predictions = gradient_boosting_classifier.predict(X_test)\n",
        "print(\"Train set accuracy is: {} and test set accuracy is: {}\".format(round(accuracy_score(y_train, train_predictions), 6),\n",
        "                                                                      round(accuracy_score(y_test, test_predictions), 6)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHTYtN0oIQXX"
      },
      "source": [
        "### 3.5 XGBoost with default parameters\n",
        "\n",
        "XGBoost is implemented in the `xgboost` package (not part of sklearn) but the API of `xgboost` closely follows that of sklearn.\n",
        "\n",
        "The code fragment below trains a gradiet boosting classifier with default hyper-parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIz6CwaBIOr2",
        "outputId": "38e6f968-42e9-44ef-fe96-869540f9cb68"
      },
      "outputs": [],
      "source": [
        "# Create a XGBoost classifier object\n",
        "xgboost_classifier = xgboost.XGBClassifier(random_state=seed,)\n",
        "\n",
        "# Fit the training data to the classifier\n",
        "xgboost_classifier = xgboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Calculate accuracy of the train and test sets\n",
        "train_predictions = xgboost_classifier.predict(X_train)\n",
        "test_predictions = xgboost_classifier.predict(X_test)\n",
        "print(\"Train set accuracy is: {} and test set accuracy is: {}\".format(round(accuracy_score(y_train, train_predictions), 6),\n",
        "                                                                      round(accuracy_score(y_test, test_predictions), 6)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD4iINwlM7RF"
      },
      "source": [
        "XGBoost also has several hyper-parameters, the code snippet below illustrates how they can be tuned using `GridSearchCV`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9doU5rjtKako",
        "outputId": "fcab2a1f-f2f5-47b7-8c38-4b9e07367af1"
      },
      "outputs": [],
      "source": [
        "# Define parameter space to search\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 6],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
        "    'min_child_weight': [1, 10, 100],\n",
        "    'colsample_bytree': [0.3, 0.7, 1],\n",
        "}\n",
        "\n",
        "# Create XGBoost classifier object\n",
        "xgboost_classifier = xgboost.XGBClassifier(random_state=seed,)\n",
        "\n",
        "# Perform grid search in the defined parameter space with cross validation (3 fold) -> in total 3*4*3*3*3 = 324 model fits \n",
        "CV_xgboost_classifier = GridSearchCV(estimator=xgboost_classifier, param_grid=param_grid, cv= 5)\n",
        "CV_xgboost_classifier.fit(X_train, y_train)\n",
        "print('Best Parameters:', CV_xgboost_classifier.best_params_)\n",
        "\n",
        "# Calculate accuracy of the best XGBoost classifier found by grid search\n",
        "train_predictions = CV_xgboost_classifier.predict(X_train)\n",
        "test_predictions = CV_xgboost_classifier.predict(X_test)\n",
        "print(\"Train set accuracy is: {} and test set accuracy is: {}\".format(round(accuracy_score(y_train, train_predictions), 6),\n",
        "                                                                      round(accuracy_score(y_test, test_predictions), 6)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4TMCAkmaQVV"
      },
      "source": [
        "## 4. Isolation forests\n",
        "\n",
        "***********\n",
        "\n",
        "Isolation forests are an anomaly/outlier detection technique that relies on trees.\n",
        "\n",
        "The code fragment below shows how Isolation Forests can be used to detect outliers in the Wisonsin Breast cancer datase and visualizes the result using t-SNE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        },
        "id": "0IZ3hOKpcbqt",
        "outputId": "2eb06d6c-7c5a-4a7b-8026-2472011a5e27"
      },
      "outputs": [],
      "source": [
        "# Create Pandas DataFrame from the predictor array\n",
        "predictor_df = pd.DataFrame(predictors, columns=breast_cancer_data['feature_names'])\n",
        "\n",
        "# Use T-distributed Stochastic Neighbor Embedding (TSNE) for creating low ddimensional embeddings (visualization purposes)\n",
        "predictor_embedded = TSNE(n_components=2, \n",
        "                          learning_rate=200,\n",
        "                          init='random', \n",
        "                          perplexity=10,\n",
        "                          random_state=seed,).fit_transform(predictor_df)\n",
        "predictor_embedded_df = pd.DataFrame(predictor_embedded, columns=['Component 1', 'Component 2'])\n",
        "\n",
        "# Fit and predict isolation forest model\n",
        "isolation_forest = ensemble.IsolationForest(n_estimators=1000, bootstrap=False, random_state=seed,).fit(predictor_df)\n",
        "outliers = isolation_forest.predict(predictor_df)\n",
        "outlier_scores = isolation_forest.score_samples(predictor_df)*-1\n",
        "\n",
        "# Assign the outlier labels as a column to the Pandas DataFrame\n",
        "predictor_embedded_df['outlier'] = np.where(outliers == -1, 1, np.where(outliers == 1, 0, outliers))\n",
        "predictor_embedded_df['outlier_scores'] = outlier_scores\n",
        "\n",
        "# Verbose the outlying cases indices and plot using 2 variables (outlier are marked as -1 and normal as 1)\n",
        "print('Outlier Indices: \\n', np.where(outliers == -1), '\\n')\n",
        "print('Outlier Scores: \\n', predictor_embedded_df.head(n=10), '\\n')\n",
        "fig, ax = plt.subplots(figsize=(8, 5), dpi=100)\n",
        "sns.scatterplot(x='Component 1', \n",
        "                y='Component 2', \n",
        "                hue='outlier', \n",
        "                data=predictor_embedded_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAy4erEvRQY-"
      },
      "source": [
        "## 5. Auto-ML\n",
        "\n",
        "Auto-ML is an umbrella term for techniques that automatically select and train one or several ML models without any human interaction. Auto-sklearn is an implementation of Auto-ML that scans through the library of models implemented in Sklearn and uses hyper-parameter tuning, bagging and stacking to combine these models into a large ensemble. The main input that is provided by the user is the amount of compute time the training phase is allowed to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jp036kSmHWVe",
        "outputId": "e0823555-1482-4dba-8e51-baaa45dc7e26"
      },
      "outputs": [],
      "source": [
        "!pip install auto-sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELHyXQ-WHaDk"
      },
      "outputs": [],
      "source": [
        "# Import Auto-Sklearn (Run Twice!)\n",
        "import autosklearn.classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The package `autosklearn` exposes an API that mimicks that of sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEwTYGmxY6WU",
        "outputId": "a6c43403-02c7-494e-fcf3-024433d7175b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set accuracy is: 0.995305 and test set accuracy is: 0.993007\n"
          ]
        }
      ],
      "source": [
        "# Define AutoML classification model from Auto-Sklearn\n",
        "automl = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=180)\n",
        "\n",
        "# Fit AutoML classification model \n",
        "automl.fit(X_train, y_train)\n",
        "\n",
        "# Calculate accuracy of the train and test sets\n",
        "train_predictions = automl.predict(X_train)\n",
        "test_predictions = automl.predict(X_test)\n",
        "print(\"Train set accuracy is: {} and test set accuracy is: {}\".format(round(accuracy_score(y_train, train_predictions), 6),\n",
        "                                                                      round(accuracy_score(y_test, test_predictions), 6)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBBkDU3gZTED",
        "outputId": "290ba825-610e-4c95-d431-e0fdb7e02d2c"
      },
      "outputs": [],
      "source": [
        "# Verbose Final Model Leaderboard from AutoML\n",
        "print(automl.leaderboard())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
